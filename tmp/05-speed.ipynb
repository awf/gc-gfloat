{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Graphcore Ltd. All rights reserved.\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import ml_dtypes\n",
    "import gfloat\n",
    "from gfloat.formats import format_info_ocp_e5m2\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing tests\n",
    "\n",
    "The `gfloat` library is designed for readability over performance, and the reference code for computations is the (slow) scalar code e.g. `round_float`.  There are vectorized implementations (e.g. `round_ndarray`, and when combined with JAX, these can go reasonably fast).\n",
    "\n",
    "Let's try to convert some values to FP8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "/home/awf/.micromamba/envs/gfloat-clean/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:68: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype, copy=copy, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFloat scalar                  :616 ms ± 23.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "GFloat vectorized, numpy arrays:4.49 ms ± 255 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "GFloat vectorized, JAX JIT     :596 µs ± 13.2 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "ML_dtypes                      :266 µs ± 16.6 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "N = 100_000\n",
    "a = np.random.rand(N)\n",
    "\n",
    "jax_round_jit = jax.jit(lambda x: gfloat.round_ndarray(format_info_ocp_e5m2, x, np=jnp))\n",
    "ja = jnp.array(a)\n",
    "jax_round_jit(ja)  # Cache compilation\n",
    "\n",
    "\n",
    "def slow_round_ndarray(fi, a):\n",
    "    return np.array([gfloat.round_float(fi, x) for x in a])\n",
    "\n",
    "\n",
    "print(\"GFloat scalar                  :\", end=\"\")\n",
    "%timeit slow_round_ndarray(format_info_ocp_e5m2, a)\n",
    "\n",
    "print(\"GFloat vectorized, numpy arrays:\", end=\"\")\n",
    "%timeit gfloat.round_ndarray(format_info_ocp_e5m2, a)\n",
    "\n",
    "print(\"GFloat vectorized, JAX JIT     :\", end=\"\")\n",
    "%timeit jax_round_jit(ja)\n",
    "\n",
    "print(\"ML_dtypes                      :\", end=\"\")\n",
    "%timeit a.astype(ml_dtypes.float8_e5m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On one CPU platform the timings were:\n",
    "```\n",
    "GFloat scalar                  :629     ms ± 22.3 ms \n",
    "GFloat vectorized, numpy arrays:  4.420 ms ± 153 µs \n",
    "GFloat vectorized, JAX JIT     :    585 µs ± 13.7 µs \n",
    "ML_dtypes                      :    253 µs ± 12 µs \n",
    "```\n",
    "So the JAX JIT code is 1000x faster than the scalar code, although `ml_dtypes`'s C++ is 2-3x faster still."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
