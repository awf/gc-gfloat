{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Graphcore Ltd. All rights reserved.\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import ml_dtypes\n",
    "import gfloat\n",
    "from gfloat.formats import format_info_ocp_e5m2\n",
    "from timeit import Timer\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timing tests\n",
    "\n",
    "The `gfloat` library is designed for readability over performance, and the reference code for computations is the (slow) scalar code e.g. `round_float`.\n",
    "\n",
    "There are vectorized implementations (e.g. `round_ndarray`), and when combined with JAX, these can go reasonably fast.\n",
    "\n",
    "Let's see how long it takes to encode some values to FP8..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFloat scalar                  :  6666.04 nsec (50 runs at size 10000)\n",
      "GFloat vectorized, numpy arrays:    57.84 nsec (50 runs at size 1000000)\n",
      "GFloat vectorized, JAX JIT     :     3.17 nsec (1000 runs at size 1000000)\n",
      "ML_dtypes                      :     2.92 nsec (1000 runs at size 1000000)\n"
     ]
    }
   ],
   "source": [
    "N = 1_000_000\n",
    "a = np.random.rand(N)\n",
    "\n",
    "jax_round_jit = jax.jit(lambda x: gfloat.round_ndarray(format_info_ocp_e5m2, x, np=jnp))\n",
    "ja = jnp.array(a)\n",
    "jax_round_jit(ja)  # Cache compilation\n",
    "\n",
    "\n",
    "def slow_round_ndarray(fi, a):\n",
    "    return np.array([gfloat.round_float(fi, x) for x in a])\n",
    "\n",
    "\n",
    "def time(f, problem_size=1.0):\n",
    "    units = 1e9  # nsec\n",
    "    t = Timer(f)\n",
    "    n = t.autorange()[0] * 10  # About 2 sec per run\n",
    "    ts = t.repeat(repeat=3, number=n)  # best of 3\n",
    "    ts = [((t / n) / problem_size) * units for t in ts]  # per run\n",
    "    return f\"{min(ts):8.2f} nsec ({n} runs at size {problem_size})\"\n",
    "\n",
    "\n",
    "# fmt: off\n",
    "print(\"GFloat scalar                  :\", time(lambda: slow_round_ndarray(format_info_ocp_e5m2, a[: N // 100]), N // 100))\n",
    "print(\"GFloat vectorized, numpy arrays:\", time(lambda: gfloat.round_ndarray(format_info_ocp_e5m2, a), N))\n",
    "print(\"GFloat vectorized, JAX JIT     :\", time(lambda: jax_round_jit(ja), N))\n",
    "print(\"ML_dtypes                      :\", time(lambda: a.astype(ml_dtypes.float8_e5m2), N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On one CPU platform the timings were:\n",
    "```\n",
    "GFloat scalar                  :  6996.75 nsec (50 runs at size 10000)\n",
    "GFloat vectorized, numpy arrays:    75.04 nsec (50 runs at size 1000000)\n",
    "GFloat vectorized, JAX JIT     :     3.18 nsec (1000 runs at size 1000000)\n",
    "ML_dtypes                      :     3.13 nsec (1000 runs at size 1000000)\n",
    "```\n",
    "So the JAX JIT code is ~1000x faster than the scalar code, and comparable to `ml_dtypes`'s C++ CPU implementation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
